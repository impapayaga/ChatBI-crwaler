# 政策文件智能分析系统 - 项目文档

## 📋 项目概述

**项目名称**: 政策文件智能分析系统   
**技术栈**: Python + FastAPI + SQLite + AI API  
**功能**: 自动爬取政府政策文件，使用AI进行智能总结分析  

## 🏗️ 系统架构

### 整体架构图
```
┌─────────────────────────────────────────────────────────────┐
│                    政策文件智能分析系统                        │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐     │
│  │   爬虫模块   │    │   AI总结模块  │    │   定时调度   │     │
│  │  crawler.py │    │ai_summarizer │    │ scheduler.py │     │
│  └─────────────┘    └─────────────┘    └─────────────┘     │
│         │                   │                   │          │
│         ▼                   ▼                   ▼          │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │                数据库层 (SQLite)                        │ │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐    │ │
│  │  │ 政策文件表   │  │  AI总结表   │  │  配置表     │    │ │
│  │  │PolicyFile  │  │AISummary   │  │CrawlConfig │    │ │
│  │  └─────────────┘  └─────────────┘  └─────────────┘    │ │
│  └─────────────────────────────────────────────────────────┘ │
│         │                   │                   │          │
│         ▼                   ▼                   ▼          │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │                API接口层 (FastAPI)                     │ │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐    │ │
│  │  │  数据接口   │  │  操作接口   │  │  统计接口   │    │ │
│  │  │/api/policies│  │/api/crawl  │  │/api/stats  │    │ │
│  │  └─────────────┘  └─────────────┘  └─────────────┘    │ │
│  └─────────────────────────────────────────────────────────┘ │
│         │                   │                   │          │
│         ▼                   ▼                   ▼          │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │                前端展示层 (HTML/JS)                    │ │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐    │ │
│  │  │  数据展示   │  │  操作控制   │  │  统计信息   │    │ │
│  │  │  政策列表   │  │  爬取按钮   │  │  数据统计   │    │ │
│  │  └─────────────┘  └─────────────┘  └─────────────┘    │ │
│  └─────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
```

### 技术架构说明

**后端架构**: 采用分层架构模式
- **数据层**: SQLite数据库 + SQLAlchemy ORM
- **业务层**: 爬虫模块 + AI总结模块 + 调度器模块
- **接口层**: FastAPI RESTful API

**数据流**: 爬取 → 存储 → AI总结 → API接口 → 前端展示

## 📁 文件结构说明

### 核心文件

#### 1. `main.py` - 主程序入口
**功能**:
- API服务模式 (`--mode api`)
- 定时调度模式 (`--mode scheduler`)
- 立即执行模式 (`--mode run_once`)

**启动方式**:
```bash
# API服务模式
python main.py --mode api

# 定时调度模式
python main.py --mode scheduler --schedule weekly

# 立即执行模式
python main.py --mode run_once
```

#### 2. `database.py` - 数据库模型
**作用**: 定义数据库表结构和ORM模型  
**架构**: SQLAlchemy ORM + SQLite  
**包含模型**:
- `PolicyFile`: 政策文件表
- `AISummary`: AI总结表
- `CrawlConfig`: 爬取配置表

**核心功能**:
- 数据库连接管理
- 表结构定义
- 数据模型映射

#### 3. `crawler.py` - 爬虫模块
**作用**: 爬取政府网站政策文件  
**架构**: 面向对象设计 + 请求库  
**目标网站**:
- 北京市政府: `https://www.beijing.gov.cn/zhengce/zhengcefagui/`
- 广东省政府: `https://www.gd.gov.cn/zwgk/wjk/qbwj/index.html`

**核心功能**:
- 网页内容爬取
- 关键词过滤（人工智能、医疗器械、生物医药）
- 数据清洗和去重
- 发布单位提取（特意加的模块不然提取不到发布单位。。）

#### 4. `ai_summarizer.py` - AI总结模块
**作用**: 使用AI对政策文件进行智能总结  
**架构**: HTTP客户端 + AI API  
**AI服务**: SiliconFlow API  
**模型**: Qwen/Qwen2.5-7B-Instruct

**核心功能**:
- 调用AI API进行文本总结
- 结构化输出（标题、单位、日期、摘要、链接）

#### 5. `scheduler.py` - 定时调度模块
**作用**: 定时执行爬取和AI总结任务  
**架构**: 调度器模式 + 定时任务  
**调度模式**:
- 每周执行（生产模式）
- 每日执行（测试模式）
- 每小时执行（开发模式）

**核心功能**:
- 定时任务调度
- 任务状态管理
- 错误重试机制
- 执行日志记录

#### 6. `api.py` - API接口层
**作用**: 提供RESTful API接口  
**架构**: FastAPI框架 + 依赖注入  
**接口列表**:
- `GET /api/policies` - 获取政策文件列表
- `GET /api/summaries` - 获取AI总结列表
- `GET /api/stats` - 获取统计信息
- `POST /api/crawl` - 启动爬取任务
- `POST /api/summarize` - 启动AI总结任务

### 配置文件

#### 7. `requirements.txt` - 依赖包列表
**作用**: 定义项目依赖的Python包  
**包含**: FastAPI、SQLAlchemy、requests、beautifulsoup4等

#### 8. `policy_data.db` - 数据库文件
**作用**: SQLite数据库文件，存储所有数据  
**包含表**: 政策文件表、AI总结表、配置表

### 启动脚本

#### 9. `start.bat` - 主启动脚本
**作用**: 提供交互式启动菜单  
**功能**: 启动API服务、打开前端、查看数据等

#### 10. `start_scheduler.bat` - 调度器启动脚本
**作用**: 启动每周自动爬取调度器

#### 11. `start_daily_scheduler.bat` - 每日调度器启动脚本
**作用**: 启动每日自动爬取调度器（测试模式）

### 前端文件

#### 12. `demo_frontend.html` - 前端界面
**作用**: 提供Web界面展示数据  
**架构**: HTML + CSS + JavaScript  
**功能**: 数据展示、操作控制、统计信息

### 工具文件

#### 13. `check_data.py` - 数据检查工具
**作用**: 检查数据库中的数据质量和统计信息

## 🚀 后端启动方式

### 1. API服务模式（推荐）
```bash
# 启动API服务
python main.py --mode api

# 或使用启动脚本
start.bat
```

**功能**: 提供RESTful API接口，支持前端调用

### 2. 定时调度模式
```bash
# 每周自动爬取
python main.py --mode scheduler --schedule weekly

# 每日自动爬取（测试）
python main.py --mode scheduler --schedule daily

# 每小时自动爬取（开发）
python main.py --mode scheduler --schedule hourly
```

**功能**: 定时执行爬取和AI总结任务

### 3. 立即执行模式
```bash
# 立即执行一次爬取和总结
python main.py --mode run_once
```

**功能**: 手动触发一次完整的爬取和总结流程

## 🔌 前端调用后端API

### API基础信息
- **服务地址**: `http://localhost:8000`
- **API文档**: `http://localhost:8000/docs`
- **健康检查**: `http://localhost:8000/api/stats`

### 核心API接口

#### 1. 获取政策文件列表
```javascript
// 请求
GET http://localhost:8000/api/policies

// 响应
{
  "policies": [
    {
      "id": 1,
      "title": "政策文件标题",
      "source": "北京市政府",
      "url": "https://...",
      "publish_date": "2024-01-01",
      "publish_unit": "北京市科学技术委员会",
      "keywords_found": "人工智能",
      "content": "政策内容...",
      "created_at": "2024-01-01T00:00:00"
    }
  ]
}
```

#### 2. 获取AI总结列表
```javascript
// 请求
GET http://localhost:8000/api/summaries

// 响应
{
  "summaries": [
    {
      "id": 1,
      "title": "政策文件标题",
      "publish_unit": "北京市科学技术委员会",
      "publish_date": "2024-01-01",
      "summary": "AI生成的总结内容...",
      "url": "https://...",
      "created_at": "2024-01-01T00:00:00"
    }
  ]
}
```

#### 3. 获取统计信息
```javascript
// 请求
GET http://localhost:8000/api/stats

// 响应
{
  "total_policies": 23,
  "total_summaries": 22,
  "beijing_policies": 19,
  "guangdong_policies": 4
}
```

#### 4. 启动爬取任务
```javascript
// 请求
POST http://localhost:8000/api/crawl

// 响应
{
  "success": true,
  "message": "爬取完成，新增 5 条政策文件",
  "total_found": 5
}
```

#### 5. 启动AI总结任务
```javascript
// 请求
POST http://localhost:8000/api/summarize

// 响应
{
  "success": true,
  "message": "AI总结完成，新增 3 条总结",
  "processed_count": 3
}
```

### 前端调用示例

#### JavaScript调用示例
```javascript
// 获取政策文件数据
async function loadPolicies() {
    try {
        const response = await fetch('http://localhost:8000/api/policies');
        const data = await response.json();
        console.log('政策文件:', data);
        return data;
    } catch (error) {
        console.error('获取数据失败:', error);
    }
}

// 获取AI总结数据
async function loadSummaries() {
    try {
        const response = await fetch('http://localhost:8000/api/summaries');
        const data = await response.json();
        console.log('AI总结:', data);
        return data;
    } catch (error) {
        console.error('获取总结失败:', error);
    }
}

// 启动爬取任务
async function startCrawl() {
    try {
        const response = await fetch('http://localhost:8000/api/crawl', {
            method: 'POST'
        });
        const result = await response.json();
        console.log('爬取结果:', result);
        return result;
    } catch (error) {
        console.error('爬取失败:', error);
    }
}

// 启动AI总结任务
async function startSummarize() {
    try {
        const response = await fetch('http://localhost:8000/api/summarize', {
            method: 'POST'
        });
        const result = await response.json();
        console.log('总结结果:', result);
        return result;
    } catch (error) {
        console.error('总结失败:', error);
    }
}
```

#### Python调用示例
```python
import requests

# 获取政策文件数据
def get_policies():
    response = requests.get('http://localhost:8000/api/policies')
    return response.json()

# 获取AI总结数据
def get_summaries():
    response = requests.get('http://localhost:8000/api/summaries')
    return response.json()

# 启动爬取任务
def start_crawl():
    response = requests.post('http://localhost:8000/api/crawl')
    return response.json()

# 启动AI总结任务
def start_summarize():
    response = requests.post('http://localhost:8000/api/summarize')
    return response.json()
```

## 📊 数据库结构

### 政策文件表 (PolicyFile)
```sql
CREATE TABLE policy_files (
    id INTEGER PRIMARY KEY,
    title TEXT NOT NULL,
    source TEXT NOT NULL,
    url TEXT NOT NULL,
    publish_date TEXT,
    publish_unit TEXT,
    keywords_found TEXT,
    content TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### AI总结表 (AISummary)
```sql
CREATE TABLE ai_summaries (
    id INTEGER PRIMARY KEY,
    policy_id INTEGER,
    title TEXT NOT NULL,
    publish_unit TEXT,
    publish_date TEXT,
    summary TEXT,
    url TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (policy_id) REFERENCES policy_files (id)
);
```

### 配置表 (CrawlConfig)
```sql
CREATE TABLE crawl_configs (
    id INTEGER PRIMARY KEY,
    website_name TEXT NOT NULL,
    website_url TEXT NOT NULL,
    keywords TEXT NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    last_crawl TIMESTAMP
);
```

## 🔧 部署说明

### 开发环境部署
```bash
# 1. 安装依赖
pip install -r requirements.txt

# 2. 启动API服务
python main.py --mode api

# 3. 打开前端
start demo_frontend.html
```

### 生产环境部署
```bash
# 1. 启动定时调度器
python main.py --mode scheduler --schedule weekly

# 2. 启动API服务（新窗口）
python main.py --mode api
```

## 📈 性能优化

### 爬取优化
- **请求间隔**: 1-2秒，避免对目标网站造成压力
- **超时设置**: 30秒，确保网络异常时能及时处理
- **重试机制**: 3次重试，提高成功率
- **并发控制**: 单线程爬取，避免被封IP

### 数据库优化
- **索引优化**: 自动创建必要索引
- **查询优化**: 使用ORM查询，避免SQL注入
- **连接池**: 自动连接管理
- **事务控制**: 自动事务管理



### 日志查看
```bash
# 查看系统日志
python main.py --mode api 2>&1 | tee system.log

# 查看错误日志
grep "ERROR" system.log
```

### 数据重置
```bash
# 删除数据库文件（会丢失所有数据）
rm policy_data.db

# 重新启动系统
python main.py --mode api
```

