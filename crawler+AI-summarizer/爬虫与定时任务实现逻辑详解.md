# 数据爬取和定时任务实现逻辑详解

## 📋 目录
1. [技术栈概览](#技术栈概览)
2. [爬虫实现逻辑](#爬虫实现逻辑)
3. [定时任务实现逻辑](#定时任务实现逻辑)
4. [AI总结实现逻辑](#ai总结实现逻辑)
5. [完整工作流程](#完整工作流程)
6. [关键技术细节](#关键技术细节)

---

## 🔧 技术栈概览

### 核心依赖库
```python
# HTTP请求
requests==2.31.0          # 发送HTTP请求，获取网页内容
httpx==0.25.2             # 异步HTTP客户端（用于AI API调用）

# HTML解析
beautifulsoup4==4.12.2    # HTML解析和内容提取
lxml==4.9.3                # XML/HTML解析器（BeautifulSoup后端）

# 定时任务
schedule==1.2.0           # Python定时任务调度库

# Web框架
fastapi==0.104.1          # 异步Web框架
uvicorn==0.24.0           # ASGI服务器

# 数据库
sqlalchemy==2.0.23        # ORM框架
sqlite3                   # 轻量级数据库（Python内置）

# 其他
pydantic==2.5.0           # 数据验证
python-multipart==0.0.6   # 文件上传支持
```

---

## 🕷️ 爬虫实现逻辑

### 1. 爬虫类架构 (`PolicyCrawler`)

#### 1.1 初始化
```python
class PolicyCrawler:
    def __init__(self):
        # 创建HTTP会话，复用连接
        self.session = requests.Session()
        
        # 设置请求头，模拟浏览器
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ...'
        })
        
        # 关键词过滤列表
        self.keywords = ["人工智能", "医疗器械", "生物医药"]
```

**关键点**：
- 使用 `requests.Session()` 复用TCP连接，提高效率
- 设置User-Agent模拟浏览器，避免被反爬虫拦截
- 关键词列表用于过滤相关政策文件

#### 1.2 关键词过滤机制

```python
def contains_keywords(self, text):
    """检查文本是否包含关键词"""
    found_keywords = []
    for keyword in self.keywords:
        if keyword in text:
            found_keywords.append(keyword)
    return len(found_keywords) > 0, found_keywords
```

**实现方法**：
- **简单字符串匹配**：使用 `in` 操作符检查关键词
- **返回匹配结果**：返回是否匹配和匹配到的关键词列表
- **灵活性**：可以匹配多个关键词，只要有一个匹配就返回True

---

### 2. 北京市政府网站爬取 (`crawl_beijing_policies`)

#### 2.1 分页URL构建

```python
# 构建分页URL列表
base_urls = []
base_url = "https://www.beijing.gov.cn/zhengce/zhengcefagui/"

# 第一页
base_urls.append(base_url)

# 其他页面（第2页到第10页）
for i in range(2, max_pages + 1):
    if i == 2:
        page_url = base_url + "index_2.html"
    else:
        page_url = base_url + f"index_{i}.html"
    base_urls.append(page_url)
```

**实现方法**：
- **静态URL模式**：北京市政府网站使用固定的分页URL格式
- **第一页**：直接使用基础URL
- **其他页**：使用 `index_2.html`, `index_3.html` 等格式

#### 2.2 HTML解析流程

```python
# 1. 发送HTTP请求
response = self.session.get(base_url, timeout=30)
response.encoding = 'utf-8'

# 2. 使用BeautifulSoup解析HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 3. 查找所有链接
all_links = soup.find_all('a', href=True)
```

**技术栈**：
- **BeautifulSoup**：Python最流行的HTML解析库
- **解析器**：使用 `html.parser`（Python内置，无需额外依赖）
- **选择器**：`find_all('a', href=True)` 查找所有带href属性的链接

#### 2.3 链接过滤和URL构建

```python
for link in all_links:
    title = link.get_text(strip=True)  # 获取链接文本（标题）
    href = link.get('href', '')        # 获取链接地址
    
    # 关键词过滤
    has_keywords, found_keywords = self.contains_keywords(title)
    if not has_keywords:
        continue  # 跳过不包含关键词的链接
    
    # 构建完整URL
    if href.startswith('/'):
        full_url = "https://www.beijing.gov.cn" + href
    elif href.startswith('http'):
        full_url = href
    else:
        full_url = base_url + href
```

**URL处理逻辑**：
- **绝对路径**（`/path`）：拼接域名
- **绝对URL**（`http://...`）：直接使用
- **相对路径**（`path`）：拼接基础URL

#### 2.4 去重机制

```python
# 检查是否已存在
existing = db.query(PolicyFile).filter(
    PolicyFile.url == full_url
).first()
if existing:
    continue  # 跳过已存在的记录
```

**实现方法**：
- **数据库查询**：使用SQLAlchemy ORM查询
- **唯一性检查**：基于URL判断是否已爬取
- **避免重复**：如果URL已存在，跳过该链接

#### 2.5 详细信息提取

**步骤1：获取详情页面**
```python
detail_response = self.session.get(full_url, timeout=30)
detail_soup = BeautifulSoup(detail_response.text, 'html.parser')
page_text = detail_soup.get_text()  # 获取页面纯文本
```

**步骤2：日期提取（正则表达式）**
```python
date_patterns = [
    r'发布日期[：:]\s*(\d{4}-\d{1,2}-\d{1,2})',
    r'发布时间[：:]\s*(\d{4}-\d{1,2}-\d{1,2})',
    r'发文时间[：:]\s*(\d{4}-\d{1,2}-\d{1,2})',
    r'(\d{4}年\d{1,2}月\d{1,2}日)',
    r'(\d{4}-\d{1,2}-\d{1,2})',
    r'(\d{4}/\d{1,2}/\d{1,2})'
]

for pattern in date_patterns:
    date_match = re.search(pattern, page_text)
    if date_match:
        publish_date = date_match.group(1)
        # 标准化日期格式
        if '年' in publish_date:
            # 转换 "2024年1月1日" 为 "2024-01-01"
            date_parts = re.findall(r'(\d{4})年(\d{1,2})月(\d{1,2})日', publish_date)
            if date_parts:
                year, month, day = date_parts[0]
                publish_date = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        break
```

**实现方法**：
- **多模式匹配**：使用多个正则表达式模式，提高匹配成功率
- **格式标准化**：将不同格式的日期统一转换为 `YYYY-MM-DD` 格式
- **正则表达式**：Python `re` 模块进行模式匹配

**步骤3：发布单位提取**
```python
unit_patterns = [
    r'发布单位[：:]\s*([^\n\r，,。；;]+)',
    r'发文机关[：:]\s*([^\n\r，,。；;]+)',
    r'北京市[^，,。\n\r]{2,30}(?:委员会|局|厅|办公室|政府|部门)',
    # ... 更多模式
]

for pattern in unit_patterns:
    unit_match = re.search(pattern, page_text)
    if unit_match:
        publish_unit = unit_match.group(1).strip()
        break
```

**实现方法**：
- **正则表达式捕获组**：`([^\n\r，,。；;]+)` 匹配到换行符或标点符号为止
- **多模式尝试**：依次尝试多个模式，找到第一个匹配的
- **字符串清理**：使用 `strip()` 去除首尾空白

#### 2.6 内容提取和保存

```python
# 提取内容（限制长度）
content = detail_soup.get_text()
content = content[:5000]  # 限制为5000字符

# 保存到数据库
policy = PolicyFile(
    title=title,
    source="北京市政府",
    url=full_url,
    publish_date=publish_date,
    publish_unit=publish_unit,
    content=content,
    keywords_found=','.join(found_keywords)
)
db.add(policy)
db.commit()
```

**实现方法**：
- **SQLAlchemy ORM**：使用ORM对象操作数据库
- **事务管理**：批量添加后统一提交，提高效率

#### 2.7 请求频率控制

```python
time.sleep(1)  # 避免请求过快
```

**实现方法**：
- **时间延迟**：每次请求后暂停1秒
- **反爬虫策略**：避免触发网站的反爬虫机制

---

### 3. 广东省政府网站爬取 (`crawl_guangdong_policies`)

#### 3.1 与北京市爬取的区别

1. **分页URL不同**：
   ```python
   base_urls = [
       "https://www.gd.gov.cn/zwgk/wjk/qbwj/index.html",
       "https://www.gd.gov.cn/zwgk/wjk/qbwj/index_2.html",
       # ...
   ]
   ```

2. **编码处理**：
   ```python
   # 使用更健壮的编码处理
   soup = BeautifulSoup(response.content, 'html.parser')
   response.encoding = response.apparent_encoding or 'utf-8'
   ```

3. **日期提取策略**：
   - 首先从标题中提取日期
   - 如果失败，从URL中提取（`/2024/01/01/` 格式）
   - 最后从网页内容中提取

4. **请求频率**：
   ```python
   time.sleep(0.5)  # 广东省网站使用0.5秒延迟
   ```

---

## ⏰ 定时任务实现逻辑

### 1. 定时任务库：`schedule`

**schedule库特点**：
- **轻量级**：纯Python实现，无需额外依赖
- **简单易用**：API简洁直观
- **单线程**：在主线程中运行，需要轮询检查

### 2. 调度器类架构 (`PolicyScheduler`)

```python
class PolicyScheduler:
    def __init__(self):
        self.crawler = PolicyCrawler()      # 爬虫实例
        self.summarizer = AISummarizer()     # AI总结实例
```

**设计模式**：
- **组合模式**：调度器包含爬虫和总结器实例
- **单一职责**：调度器只负责调度，不负责具体业务逻辑

### 3. 定时任务执行流程

#### 3.1 核心任务方法

```python
def scheduled_crawl_and_summarize(self):
    """定时爬取和总结任务"""
    # 1. 获取数据库会话
    db = next(get_db())
    
    # 2. 执行爬取
    total_found = self.crawler.crawl_all_policies(db)
    
    # 3. 执行AI总结
    processed_count = self.summarizer.process_unprocessed_policies(db)
    
    # 4. 更新配置中的最后爬取时间
    configs = db.query(CrawlConfig).all()
    for config in configs:
        config.last_crawl = datetime.utcnow()
    db.commit()
```

**执行顺序**：
1. **爬取** → 2. **总结** → 3. **更新配置**

#### 3.2 调度模式配置

```python
def start_scheduler(self, schedule_type="weekly"):
    if schedule_type == "weekly":
        # 每周一上午9点执行
        schedule.every().monday.at("09:00").do(self.scheduled_crawl_and_summarize)
        
    elif schedule_type == "daily":
        # 每天上午9点执行（测试用）
        schedule.every().day.at("09:00").do(self.scheduled_crawl_and_summarize)
        
    elif schedule_type == "hourly":
        # 每6小时执行一次（测试用）
        schedule.every(6).hours.do(self.scheduled_crawl_and_summarize)
```

**schedule库API**：
- `schedule.every().monday.at("09:00")` - 每周一9点
- `schedule.every().day.at("09:00")` - 每天9点
- `schedule.every(6).hours` - 每6小时

#### 3.3 调度器运行循环

```python
# 运行调度器
while True:
    schedule.run_pending()  # 检查是否有待执行的任务
    time.sleep(60)          # 每分钟检查一次
```

**实现原理**：
- **轮询机制**：每60秒检查一次是否有待执行的任务
- **阻塞运行**：主线程一直运行，直到手动停止（Ctrl+C）
- **精确度**：受检查间隔影响，最多延迟60秒

**时间精度**：
- 理论上：精确到分钟级别
- 实际：由于轮询间隔，可能有1-60秒的延迟

---

## 🤖 AI总结实现逻辑

### 1. AI总结器架构 (`AISummarizer`)

```python
class AISummarizer:
    def __init__(self):
        self.api_key = "sk-xxx"
        self.api_url = "https://api.siliconflow.cn/v1/chat/completions"
        self.model = "Qwen/Qwen2.5-7B-Instruct"
```

**API配置**：
- **SiliconFlow API**：国内AI服务提供商
- **模型**：Qwen/Qwen2.5-7B-Instruct（通义千问）

### 2. 提示词构建

```python
def create_summary_prompt(self, policy: PolicyFile) -> str:
    prompt = f"""
请对以下政策文件进行结构化总结，要求包含以下五个方面：

政策文件信息：
- 文件名称：{policy.title}
- 发布单位：{policy.publish_unit or '未知'}
- 发布日期：{policy.publish_date or '未知'}
- 来源网站：{policy.source}
- 原始链接：{policy.url}
- 政策内容：{policy.content[:2000] if policy.content else '无详细内容'}

请按照以下格式输出JSON结果：
{{
    "title": "政策文件名称",
    "publish_unit": "发布单位",
    "publish_date": "发布日期",
    "summary": "政策摘要内容总结（200-300字）",
    "url": "超链接网址"
}}
"""
    return prompt
```

**提示词设计**：
- **结构化输入**：包含所有必要信息
- **格式化输出**：要求返回JSON格式
- **内容限制**：政策内容限制为2000字符，避免超出模型上下文

### 3. API调用

```python
def call_ai_api(self, prompt: str) -> str:
    headers = {
        "Authorization": f"Bearer {self.api_key}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": self.model,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,      # 低温度，更确定性输出
        "max_tokens": 1000        # 最大输出长度
    }
    
    with httpx.Client(timeout=30.0) as client:
        response = client.post(self.api_url, headers=headers, json=data)
        return response.json()["choices"][0]["message"]["content"]
```

**实现方法**：
- **httpx库**：异步HTTP客户端（同步模式使用）
- **超时设置**：30秒超时，避免长时间等待
- **参数配置**：
  - `temperature=0.3`：低温度，输出更确定性
  - `max_tokens=1000`：限制输出长度

### 4. 响应解析

```python
def parse_ai_response(self, response: str) -> Dict:
    # 尝试提取JSON部分
    if "```json" in response:
        # 提取代码块中的JSON
        json_start = response.find("```json") + 7
        json_end = response.find("```", json_start)
        json_str = response[json_start:json_end].strip()
    elif "{" in response and "}" in response:
        # 提取第一个JSON对象
        json_start = response.find("{")
        json_end = response.rfind("}") + 1
        json_str = response[json_start:json_end]
    else:
        # 返回默认格式
        return {"title": "解析失败", ...}
    
    return json.loads(json_str)
```

**解析策略**：
1. **代码块格式**：提取 ````json ... ``` ` 中的内容
2. **JSON对象格式**：提取第一个 `{...}` 中的内容
3. **容错处理**：如果解析失败，返回默认格式

### 5. 批量处理

```python
def process_unprocessed_policies(self, db: Session) -> int:
    # 获取未处理的政策文件
    unprocessed_policies = db.query(PolicyFile).filter(
        PolicyFile.is_processed == False
    ).all()
    
    processed_count = 0
    for policy in unprocessed_policies:
        ai_summary = self.summarize_policy(policy)
        if ai_summary:
            db.add(ai_summary)
            policy.is_processed = True  # 标记为已处理
            processed_count += 1
    
    db.commit()
    return processed_count
```

**处理逻辑**：
- **状态标记**：使用 `is_processed` 字段标记是否已处理
- **批量提交**：所有记录处理完成后统一提交
- **错误隔离**：单个政策处理失败不影响其他政策

---

## 🔄 完整工作流程

### 流程图

```
┌─────────────────────────────────────────────────────────┐
│              定时任务调度器 (PolicyScheduler)            │
│                                                         │
│  schedule.every().monday.at("09:00")                   │
│         │                                               │
│         ▼                                               │
│  scheduled_crawl_and_summarize()                       │
└─────────────────────────────────────────────────────────┘
         │
         ├─────────────────────────────────────┐
         │                                     │
         ▼                                     ▼
┌──────────────────────┐          ┌──────────────────────┐
│   爬虫模块            │          │   AI总结模块         │
│  (PolicyCrawler)       │          │  (AISummarizer)      │
│                       │          │                      │
│  1. 构建分页URL       │          │  1. 获取未处理政策   │
│  2. 发送HTTP请求      │          │  2. 构建提示词       │
│  3. 解析HTML          │          │  3. 调用AI API       │
│  4. 关键词过滤        │          │  4. 解析JSON响应     │
│  5. 提取详细信息      │          │  5. 保存到数据库     │
│  6. 去重检查          │          │  6. 标记为已处理     │
│  7. 保存到数据库      │          │                      │
└──────────────────────┘          └──────────────────────┘
         │                                     │
         └─────────────────┬───────────────────┘
                          ▼
                ┌──────────────────┐
                │   SQLite数据库    │
                │                   │
                │  PolicyFile表     │
                │  AISummary表      │
                │  CrawlConfig表    │
                └──────────────────┘
```

### 详细执行步骤

#### 步骤1：定时任务触发
```python
# 每周一上午9:00，schedule库调用
scheduled_crawl_and_summarize()
```

#### 步骤2：爬取阶段
```python
# 2.1 初始化爬虫
crawler = PolicyCrawler()

# 2.2 爬取北京市政府
crawl_beijing_policies(db, max_pages=10)
  ├─ 构建10个分页URL
  ├─ 遍历每个URL
  ├─ 解析HTML，提取所有链接
  ├─ 关键词过滤（人工智能、医疗器械、生物医药）
  ├─ 去重检查（基于URL）
  ├─ 获取详情页面
  ├─ 提取发布日期、发布单位、内容
  └─ 保存到数据库

# 2.3 爬取广东省政府
crawl_guangdong_policies(db, max_pages=10)
  └─ 类似流程...
```

#### 步骤3：AI总结阶段
```python
# 3.1 初始化总结器
summarizer = AISummarizer()

# 3.2 批量处理未总结的政策
process_unprocessed_policies(db)
  ├─ 查询 is_processed=False 的政策
  ├─ 遍历每个政策
  ├─ 构建提示词（包含政策信息）
  ├─ 调用AI API（SiliconFlow）
  ├─ 解析JSON响应
  ├─ 保存AI总结到数据库
  └─ 标记政策为已处理
```

#### 步骤4：更新配置
```python
# 更新最后爬取时间
configs = db.query(CrawlConfig).all()
for config in configs:
    config.last_crawl = datetime.utcnow()
db.commit()
```

---

## 🔑 关键技术细节

### 1. HTTP请求优化

**Session复用**：
```python
self.session = requests.Session()
```
- **连接复用**：复用TCP连接，减少握手开销
- **Cookie管理**：自动管理Cookie
- **性能提升**：减少网络延迟

**请求头设置**：
```python
self.session.headers.update({
    'User-Agent': 'Mozilla/5.0 ...'
})
```
- **反爬虫规避**：模拟真实浏览器
- **避免被拦截**：部分网站会检查User-Agent

### 2. HTML解析优化

**编码处理**：
```python
response.encoding = 'utf-8'  # 明确指定编码
# 或
response.encoding = response.apparent_encoding or 'utf-8'  # 自动检测
```

**解析器选择**：
- `html.parser`：Python内置，无需额外依赖
- `lxml`：更快，但需要额外安装

### 3. 正则表达式模式

**日期匹配**：
```python
r'(\d{4}-\d{1,2}-\d{1,2})'  # 匹配 2024-1-1 或 2024-01-01
r'(\d{4}年\d{1,2}月\d{1,2}日)'  # 匹配 2024年1月1日
```

**单位匹配**：
```python
r'发布单位[：:]\s*([^\n\r，,。；;]+)'  # 匹配到换行符或标点为止
```

### 4. 数据库操作优化

**批量提交**：
```python
# 批量添加
for policy in policies:
    db.add(policy)

# 统一提交
db.commit()
```
- **减少事务开销**：批量操作后统一提交
- **提高性能**：减少数据库往返次数

**去重策略**：
```python
existing = db.query(PolicyFile).filter(
    PolicyFile.url == full_url
).first()
```
- **唯一性检查**：基于URL字段
- **避免重复数据**：提高数据质量

### 5. 错误处理和容错

**异常捕获**：
```python
try:
    # 爬取逻辑
except Exception as e:
    logger.warning(f"处理失败: {e}")
    continue  # 继续处理下一个
```

**错误隔离**：
- 单个政策处理失败不影响其他政策
- 记录错误日志，便于排查

### 6. 定时任务精度

**轮询间隔**：
```python
time.sleep(60)  # 每分钟检查一次
```
- **精度**：最多延迟60秒
- **资源消耗**：CPU占用低

**替代方案**：
- **APScheduler**：更精确的定时任务库
- **Celery**：分布式任务队列
- **系统Cron**：操作系统级别的定时任务

---

## 📊 性能和数据统计

### 爬取性能
- **北京市政府**：每页约1-2分钟，10页约10-20分钟
- **广东省政府**：每页约0.5-1分钟，10页约5-10分钟
- **总耗时**：约15-30分钟（取决于网络速度和页面数量）

### AI总结性能
- **单条政策**：约3-5秒（API响应时间）
- **批量处理**：100条政策约5-8分钟
- **API限制**：受SiliconFlow API速率限制

### 数据存储
- **SQLite数据库**：轻量级，适合中小规模数据
- **文件大小**：通常几MB到几十MB
- **扩展性**：可迁移到PostgreSQL/MySQL

---

## 🎯 总结

### 核心实现方法
1. **HTTP请求**：`requests` 库 + Session复用
2. **HTML解析**：`BeautifulSoup` + 正则表达式
3. **定时任务**：`schedule` 库 + 轮询机制
4. **AI调用**：`httpx` + SiliconFlow API
5. **数据存储**：SQLAlchemy ORM + SQLite

### 设计特点
- **模块化设计**：爬虫、总结器、调度器分离
- **容错处理**：单个失败不影响整体
- **可扩展性**：易于添加新的数据源
- **简单易用**：代码结构清晰，易于维护

### 改进建议
1. **异步爬取**：使用 `aiohttp` 提高并发性能
2. **更精确的定时**：使用 `APScheduler` 替代 `schedule`
3. **分布式爬取**：使用 `Scrapy` 框架
4. **数据库升级**：迁移到PostgreSQL支持更大规模




