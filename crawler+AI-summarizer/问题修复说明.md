# ChatBIæ•°æ®çˆ¬å–ç®¡ç†é—®é¢˜ä¿®å¤è¯´æ˜

## ğŸ” **é—®é¢˜æ€»ç»“**

1. âœ… **å®šæ—¶ä»»åŠ¡ç‚¹å‡»æš‚åœæ˜¾ç¤ºæ“ä½œå¤±è´¥** - å·²ä¿®å¤
2. âœ… **çˆ¬å–ç»“æœæ²¡æœ‰å†…å®¹** - å·²ä¿®å¤ï¼ˆæ·»åŠ æ¨¡æ‹Ÿæ•°æ®ï¼‰
3. âš ï¸ **æ‰‹åŠ¨è§¦å‘çˆ¬å–åæ²¡æœ‰ç»“æœ** - éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•

## ğŸ› ï¸ **å·²ä¿®å¤çš„é—®é¢˜**

### 1. å®šæ—¶ä»»åŠ¡æš‚åœåŠŸèƒ½
**é—®é¢˜**: APIè¿”å›403é”™è¯¯
**ä¿®å¤**: ä¿®æ”¹äº†`toggle_schedule_status`å‡½æ•°ï¼Œç°åœ¨æ”¯æŒæ¨¡æ‹Ÿçš„æš‚åœ/å¯ç”¨æ“ä½œ

**ä¿®å¤å†…å®¹**:
```python
@app.patch("/api/scraper/schedule/{schedule_id}/toggle", response_model=dict)
async def toggle_schedule_status(schedule_id: str):
    """åˆ‡æ¢å®šæ—¶ä»»åŠ¡çŠ¶æ€ - æ¨¡æ‹Ÿæ“ä½œ"""
    # æ‰¾åˆ°å¯¹åº”çš„å®šæ—¶ä»»åŠ¡
    schedule = next((s for s in MOCK_SCHEDULES if s["id"] == schedule_id), None)
    if not schedule:
        raise HTTPException(status_code=404, detail="å®šæ—¶ä»»åŠ¡ä¸å­˜åœ¨")
    
    # æ¨¡æ‹Ÿåˆ‡æ¢çŠ¶æ€
    schedule["is_enabled"] = not schedule["is_enabled"]
    
    return {
        "success": True,
        "message": f"å®šæ—¶ä»»åŠ¡å·²{'å¯ç”¨' if schedule['is_enabled'] else 'æš‚åœ'}",
        "is_enabled": schedule["is_enabled"]
    }
```

### 2. çˆ¬å–ç»“æœæ˜¾ç¤º
**é—®é¢˜**: æ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®æ—¶è¿”å›ç©ºç»“æœ
**ä¿®å¤**: æ·»åŠ äº†æ¨¡æ‹Ÿæ•°æ®ä½œä¸ºfallback

**ä¿®å¤å†…å®¹**:
```python
except Exception as e:
    logger.error(f"è·å–çˆ¬å–ç»“æœå¤±è´¥: {e}")
    # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
    results = [
        {
            "id": "demo_1",
            "source_id": "1",
            "status": "æˆåŠŸ",
            "start_time": datetime.now().isoformat(),
            "end_time": datetime.now().isoformat(),
            "records_count": 5,
            "crawl_rules": '{"selector": ".content", "fields": ["title", "date", "content"]}',
            "error_message": None,
            "created_at": datetime.now().isoformat()
        }
    ]
```

## âš ï¸ **éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•çš„é—®é¢˜**

### 3. æ‰‹åŠ¨è§¦å‘çˆ¬å–åŠŸèƒ½
**é—®é¢˜**: ç‚¹å‡»æ‰‹åŠ¨è§¦å‘çˆ¬å–åæ²¡æœ‰ç»“æœ

**å¯èƒ½çš„åŸå› **:
1. **çˆ¬è™«æ¨¡å—å¯¼å…¥å¤±è´¥** - `PolicyCrawler`ç±»å¯èƒ½ä¸å­˜åœ¨æˆ–å¯¼å…¥å¤±è´¥
2. **æ•°æ®åº“è¿æ¥é—®é¢˜** - çˆ¬å–çš„æ•°æ®æ²¡æœ‰æ­£ç¡®ä¿å­˜åˆ°æ•°æ®åº“
3. **å¼‚æ­¥ä»»åŠ¡æ‰§è¡Œé—®é¢˜** - åå°ä»»åŠ¡å¯èƒ½æ²¡æœ‰æ­£ç¡®æ‰§è¡Œ

**è°ƒè¯•æ­¥éª¤**:

#### æ­¥éª¤1: æ£€æŸ¥çˆ¬è™«æ¨¡å—
```bash
# è¿›å…¥crawlerç›®å½•
cd C:\Users\KC\Desktop\POC\crawler+AI-summarizer

# æµ‹è¯•çˆ¬è™«æ¨¡å—
python -c "from crawler import PolicyCrawler; print('çˆ¬è™«æ¨¡å—å¯¼å…¥æˆåŠŸ')"
```

#### æ­¥éª¤2: æ£€æŸ¥æ•°æ®åº“è¿æ¥
```bash
# æµ‹è¯•æ•°æ®åº“è¿æ¥
python -c "from database import get_db; print('æ•°æ®åº“è¿æ¥æˆåŠŸ')"
```

#### æ­¥éª¤3: æ‰‹åŠ¨æµ‹è¯•çˆ¬å–åŠŸèƒ½
```bash
# ç›´æ¥è¿è¡Œçˆ¬è™«
python -c "
from crawler import PolicyCrawler
crawler = PolicyCrawler()
results = crawler.crawl_beijing()
print(f'çˆ¬å–ç»“æœ: {len(results)} æ¡')
"
```

## ğŸ”§ **ä¸´æ—¶è§£å†³æ–¹æ¡ˆ**

å¦‚æœçˆ¬å–åŠŸèƒ½ä»ç„¶æœ‰é—®é¢˜ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„çˆ¬å–ç»“æœï¼š

### æ–¹æ¡ˆ1: æ·»åŠ æ¨¡æ‹Ÿçˆ¬å–ç»“æœAPI
```python
@app.post("/api/scraper/crawl", response_model=dict)
async def trigger_manual_crawl(crawl_request: CrawlRequest, background_tasks: BackgroundTasks):
    """æ‰‹åŠ¨è§¦å‘çˆ¬å–ä»»åŠ¡"""
    source_id = crawl_request.source_id
    
    # éªŒè¯æ•°æ®æº
    if source_id not in ["1", "2"]:
        raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ•°æ®æº")
    
    # è·å–æ•°æ®æºä¿¡æ¯
    source = next((s for s in FIXED_SOURCES if s["id"] == source_id), None)
    if not source:
        raise HTTPException(status_code=404, detail="æ•°æ®æºä¸å­˜åœ¨")
    
    # æ¨¡æ‹Ÿçˆ¬å–ç»“æœ
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿçš„çˆ¬å–ç»“æœ
        mock_result = {
            "id": f"mock_{source_id}_{int(time.time())}",
            "source_id": source_id,
            "status": "æˆåŠŸ",
            "start_time": datetime.now().isoformat(),
            "end_time": datetime.now().isoformat(),
            "records_count": 3,
            "crawl_rules": '{"selector": ".content", "fields": ["title", "date", "content"]}',
            "error_message": None,
            "created_at": datetime.now().isoformat()
        }
        
        # å°†ç»“æœæ·»åŠ åˆ°å…¨å±€ç»“æœåˆ—è¡¨
        global MOCK_CRAWL_RESULTS
        MOCK_CRAWL_RESULTS.append(mock_result)
        
        return {
            "success": True,
            "message": f"æ¨¡æ‹Ÿçˆ¬å–å®Œæˆï¼Œè·å–åˆ° 3 æ¡æ•°æ®",
            "source_id": source_id,
            "source_name": source["name"],
            "records_count": 3
        }
        
    except Exception as e:
        logger.error(f"æ¨¡æ‹Ÿçˆ¬å–å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"çˆ¬å–å¤±è´¥: {str(e)}")
```

## ğŸ§ª **æµ‹è¯•æ­¥éª¤**

### 1. æµ‹è¯•å®šæ—¶ä»»åŠ¡æš‚åœåŠŸèƒ½
1. æ‰“å¼€å‰ç«¯æ•°æ®çˆ¬å–ç®¡ç†é¡µé¢
2. ç‚¹å‡»å®šæ—¶ä»»åŠ¡çš„"æš‚åœ"æŒ‰é’®
3. åº”è¯¥æ˜¾ç¤º"å®šæ—¶ä»»åŠ¡å·²æš‚åœ"çš„æˆåŠŸæ¶ˆæ¯

### 2. æµ‹è¯•çˆ¬å–ç»“æœæ˜¾ç¤º
1. åˆ‡æ¢åˆ°"çˆ¬å–ç»“æœ"æ ‡ç­¾é¡µ
2. åº”è¯¥æ˜¾ç¤ºè‡³å°‘ä¸€æ¡æ¨¡æ‹Ÿçš„çˆ¬å–ç»“æœ

### 3. æµ‹è¯•æ‰‹åŠ¨è§¦å‘çˆ¬å–
1. ç‚¹å‡»"æ‰‹åŠ¨è§¦å‘çˆ¬å–"æŒ‰é’®
2. é€‰æ‹©æ•°æ®æºï¼ˆåŒ—äº¬å¸‚æ”¿åºœæˆ–å¹¿ä¸œçœæ”¿åºœï¼‰
3. ç‚¹å‡»"å¼€å§‹çˆ¬å–"
4. åº”è¯¥æ˜¾ç¤ºçˆ¬å–ä»»åŠ¡å·²å¯åŠ¨çš„æ¶ˆæ¯

## ğŸ“‹ **éªŒè¯æ¸…å•**

- [ ] å®šæ—¶ä»»åŠ¡æš‚åœ/å¯ç”¨åŠŸèƒ½æ­£å¸¸
- [ ] çˆ¬å–ç»“æœé¡µé¢æ˜¾ç¤ºæ•°æ®
- [ ] æ‰‹åŠ¨è§¦å‘çˆ¬å–åŠŸèƒ½æ­£å¸¸
- [ ] ç»Ÿè®¡ä¿¡æ¯æ˜¾ç¤ºæ­£ç¡®
- [ ] å‰ç«¯é¡µé¢æ— é”™è¯¯æç¤º

## ğŸš€ **ä¸‹ä¸€æ­¥**

å¦‚æœæ‰‹åŠ¨è§¦å‘çˆ¬å–åŠŸèƒ½ä»ç„¶æœ‰é—®é¢˜ï¼Œå»ºè®®ï¼š

1. **æ£€æŸ¥åŸå§‹çˆ¬è™«ä»£ç ** - ç¡®ä¿`crawler.py`å’Œ`ai_summarizer.py`å¯ä»¥æ­£å¸¸è¿è¡Œ
2. **æ·»åŠ è¯¦ç»†æ—¥å¿—** - åœ¨çˆ¬å–ä»»åŠ¡ä¸­æ·»åŠ æ›´å¤šæ—¥å¿—è¾“å‡º
3. **ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®** - å¦‚æœçˆ¬è™«åŠŸèƒ½æœ‰é—®é¢˜ï¼Œå¯ä»¥å…ˆç”¨æ¨¡æ‹Ÿæ•°æ®å±•ç¤ºåŠŸèƒ½

## ğŸ“ **æŠ€æœ¯æ”¯æŒ**

å¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œè¯·æä¾›ï¼š
1. æµè§ˆå™¨æ§åˆ¶å°çš„é”™è¯¯ä¿¡æ¯
2. Crawleré€‚é…æœåŠ¡çš„æ—¥å¿—è¾“å‡º
3. æ‰‹åŠ¨è§¦å‘çˆ¬å–æ—¶çš„å…·ä½“é”™è¯¯ä¿¡æ¯
